<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="assets/css/bootstrap.min.css">
    <title>Ling Xiao's Homepage</title>
  </head>
  

  <body>
    <div class="container" style="max-width: 1600px;">
	

<nav class="navbar navbar-expand-lg navbar-dark bg-primary">
  <a class="navbar-brand" href="index.html">Ling Xiao</a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarColor01" aria-controls="navbarColor01" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="navbarColor01">
    <ul class="navbar-nav mr-auto">
	  <li class="nav-item">
        <a class="nav-link" href="project.html">Projects</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="publication.html">Publications</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="honor.html">Honors and fundings</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="activity.html">Services</a>
      </li>
    </ul>
  </div>
</nav>
<br />
<br />
        <div class="row mb-3">
            <div class="col-sm-12">
                <!-- =========== Rich list =========== -->
		    
<h4>International Conferences (Peer-reviewed)</h4>
<ul>	
	<ol>

		<li class="mb-2">
 <a href="">ActRecognition-GPT: Utilizing Multimodal Large Language Models for Spatiotemporal Action Recognition in Nursery Videos</a>, 
  K. Watanabe, S. Masuda, <strong>L. Xiao</strong>, and T. Yamasaki,   
 <em><strong> Tğ—µğ—² ğŸ­ğ˜€ğ˜ ğ—œğ—»ğ˜ğ—²ğ—¿ğ—»ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ—®ğ—¹ ğ—ªğ—¼ğ—¿ğ—¸ğ˜€ğ—µğ—¼ğ—½ ğ—¼ğ—» ğ—™ğ—¼ğ˜‚ğ—»ğ—±ğ—®ğ˜ğ—¶ğ—¼ğ—», ğ— ğ˜‚ğ—¹ğ˜ğ—¶ğ—ºğ—¼ğ—±ğ—®ğ—¹ ğ—Ÿğ—®ğ—¿ğ—´ğ—² ğ—Ÿğ—®ğ—»ğ—´ğ˜‚ğ—®ğ—´ğ—² ğ—®ğ—»ğ—± ğ—šğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ˜ƒğ—² ğ— ğ—¼ğ—±ğ—²ğ—¹ğ˜€ ğ—³ğ—¼ğ—¿ ğ—™ğ—®ğ—°ğ—² ğ—®ğ—»ğ—± ğ—šğ—²ğ˜€ğ˜ğ˜‚ğ—¿ğ—² ğ—¥ğ—²ğ—°ğ—¼ğ—´ğ—»ğ—¶ğ˜ğ—¶ğ—¼ğ—» (FM&LLM&GM 2025), the 19th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2025)</strong></em>, accepted, 2025</a>
 </li>
 

		
				<li class="mb-2">
 <a href="">TourMLLM: A Retrieval-Augmented Multimodal Large Language Model for Multitask Learning in the Tourism Domain</a>, 
  H. Yamanishi, <strong>L. Xiao*[corresponding author]</strong>, and T. Yamasaki,   
 <em><strong> The 15th ACM International Conference on Multimedia Retrieval (ICMR)</strong></em>, accepted, 2025</a>
 </li>

		<li class="mb-2">
 <a href="">Explainable AI for Image Aesthetic Evaluation Using Vision-Language Models</a>, 
 S. Viriyavisuthisakul, S.n Yoshida, K. Shiohara, <strong>L. Xiao</strong> and T. Yamasaki, 
 <em><strong> IEEE Conference on Artificial Intelligence x Multimedia (AIxMM)</strong></em>, accepted, 2025</a>
 </li>
		
		<li class="mb-2">
 <a href="https://link.springer.com/chapter/10.1007/978-981-96-2061-6_20">LITA: LMM-guided Image-Text Alignment for Art Assessment</a>, 
 T. Sunada, K. Shiohara, <strong>L. Xiao</strong>, and T. Yamasaki,      
 <em><strong> 31st International Conference on MultiMedia Modeling (MMM25)</strong></em>, pp. 268-281, 2025 </a>
 </li>
		<li class="mb-2">
 <a href="https://dl.acm.org/doi/full/10.1145/3696409.3700273">Language-Guided Self-Supervised Video Summarization Using Text Semantic Matching Considering the Diversity of the Video</a>, 
 T. Sugihara, S. Masuda, <strong>L. Xiao*[corresponding author]</strong>, and T. Yamasaki,      
 <em><strong> ACM Multimedia Asia 2024 </strong></em>, pp. 1-1, 2024 <a href="https://github.com/sugitomoo/PDL"> [Code] </a> </a>
 </li>

		
						<li class="mb-2">
 <a href="https://ieeexplore.ieee.org/document/10849859">LLaVA-Tour: A Large-Scale Multimodal Model Specializing in Japanese Tourist Spot Prediction and Review Generation</a>, 
 H. Yamanishi, <strong>L. Xiao*[corresponding author]</strong>, and T. Yamasaki,   
 <em><strong> IEEE International Conference on Visual Communications and Image Processing </strong></em>, pp. 1-5, 2024 [Best Paper Candidate]<a href="https://github.com/HiromasaYamanishi/LLaVATour"> [Code] </a> </a>
 </li>
		
				<li class="mb-2">
 <a href="https://ceur-ws.org/Vol-3886/paper6.pdf">A Multimodal Dataset and Benchmark for Tourism Review Generation</a>, 
 H. Yamanishi, <strong>L. Xiao*[corresponding author]</strong>, and T. Yamasaki,   
 <em><strong> ACM RecSys Workshop on Recommenders in Tourism (Rectour 2024)  </strong></em>, 2024 </a>
 </li>
		
		<li class="mb-2">
 <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06776.pdf">SCOMatch: Alleviating Overtrusting in Open-set Semi-supervised Learning</a>, 
 Z. R. Wang, L. Y. Xiang, L. Huang, J. F. Mao, <strong>L. Xiao</strong>, and T. Yamasaki, 
 <em><strong> European Conference on Computer Vision (ECCV)  </strong></em>, pp. 217-233, 2024 </a>
 </li>
		
		<li class="mb-2">
 <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10647958">Adversarially robust continual learning with anti-forgetting loss</a>, 
 K. Mukai, S. Kumano, N. Michel, <strong>L. Xiao</strong>, and T. Yamasaki, 
 <em><strong> IEEE International Conference on Image Processing (ICIP)  </strong></em>, pp. 1085-1091, 2024 </a>
 </li>
		
<li class="mb-2">
 <a href="https://link.springer.com/chapter/10.1007/978-3-031-70242-6_9">E-ReaRev: Adaptive Reasoning for Question Answering over Incomplete Knowledge Graphs by Edge and Meaning Extensions</a>, 
 X.T. Ye, <strong>L. Xiao</strong>, C. Zhang, and T. Yamasaki,   
 <em><strong> The 29th International Conference on Natural Language & Information Systems (NLDB)  </strong></em>, pp. 85-95, 2024 </a>
 </li>
			
<li class="mb-2">
 <a href="https://dl.acm.org/doi/abs/10.5555/3692070.3693520">Rethinking Momentum Knowledge Distillation in Online Continual Learning</a>, 
 N. Michel, M. Wang, <strong>L. Xiao</strong>, and T. Yamasaki,   
 <em><strong> International Conference on Machine Learning (ICML)  </strong></em>, pp. 35607-35622, 2024 <a href="https://github.com/Nicolas1203/mkd_ocl"> [Code] </a> </a>
 </li>
		
			<li class="mb-2">
 <a href="https://openaccess.thecvf.com/content/CVPR2024W/CVFAD/papers/Xiao_Boosting_Fine-grained_Fashion_Retrieval_with_Relational_Knowledge_Distillation_CVPRW_2024_paper.pdf">Boosting Fine-grained Fashion Retrieval with Relational Knowledge Distillation</a>, 
  <strong>L. Xiao</strong> and T. Yamasaki,   
 <em><strong> The 7th Workshop on Computer Vision for Fashion, Art, and Design, IEEE / CVF Computer Vision and Pattern Recognition Conference (Spotlight and Poster)  </strong></em>, pp. 8229-8234, 2024 <a href="https://github.com/Dr-LingXiao/RKD"> [Code] </a>
 </li>
	<li class="mb-2">
 <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Improving_Plasticity_in_Online_Continual_Learning_via_Collaborative_Learning_CVPR_2024_paper.pdf">Improving Plasticity in Online Continual Learning via Collaborative Learning</a>, 
  M. Wang, N. Michel, <strong>L. Xiao</strong>, and T. Yamasaki,   
 <em><strong> The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)  </strong></em>, pp. 23460-23469, 2024 <a href="https://github.com/maorong-wang/CCL-DC"> [Code] </a>
 </li>
	
	<li class="mb-2">
 <a href="https://dl.acm.org/doi/pdf/10.1145/3655755.3655770">HetSpot: Analyzing Tourist Spot Popularity with Heterogeneous Graph Neural Network</a>, 
 H. Yamanishi, <strong>L. Xiao*[corresponding author]</strong>, and T. Yamasaki,   
 <em><strong> 6th International Conference on Image, Video and Signal Processing (IVSP) </strong></em>, pp. 111â€“120, 2024
 </li>
	

	<li class="mb-2">
 <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10254418">Toward a More Robust Fine-grained Fashion Retrieval</a>, 
 <strong>L. Xiao</strong>, X. F. Zhang, and T. Yamasaki,   
 <em><strong> IEEE International Conference on Multimedia Information Processing and Retrieval (MIPR) </strong></em>, pp. 1-4, 2023 <a href="https://github.com/Dr-LingXiao/GD"> [Code] </a>
 </li>
	<li class="mb-2">
 <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10254405">Learning Fashion Compatibility with Color Distortion Prediction</a>, 
 <strong>L. Xiao</strong>, X. F. Zhang, and T. Yamasaki,    
 <em><strong>IEEE International Conference on Multimedia Information Processing and Retrieval (MIPR) </strong></em>, pp. 81-84, 2023
 </li>

	<li class="mb-2">
 <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10254426">Bridging the Capacity Gap for Online Knowledge Distillation</a>, 
M. Wang, H. Yu, <strong>L. Xiao</strong>, and T. Yamasaki,   
 <em><strong>IEEE International Conference on Multimedia Information Processing and Retrieval (MIPR) </strong></em>, pp. 1-4, 2023 <a href="https://github.com/maorong-wang/ABML"> [Code] </a>
 </li>
	
 <li class="mb-2">
 <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9897313">SAT: Self-adaptive training for fashion compatibility prediction</a>, 
 <strong>L. Xiao</strong> and T. Yamasaki,  
 <em><strong>IEEE International Conference on Image Processing (ICIP)</strong></em>, pp. 2431-2435, 2022
 </li>

 <li class="mb-2">
 <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8843235">Surface defect detection using hierarchical features</a>, 
 <strong>L. Xiao</strong>, T. Huang, B. Wu, Y. Hu, and J. Zhou,
 <em><strong>International Conference on Automation Science and Engineering</strong></em>, pp. 1592-1596, 2019
 </li>	
	
 <li class="mb-2">
 <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7959483">A remote health condition monitoring system based on compressed sensing</a>, 
 J. Liu, Y. Hu, Y. Lu, Y. Wang, <strong>L. Xiao</strong>, and K. Zheng,
 <em><strong>International Conference on Mechanical, System and Control Engineering</strong></em>, pp. 262-266, 2017
 </li>	
	</ol>
 
</ul>		    

<h4>International Journals (Peer-reviewed)</h4>
<ul>	
	<ol>
				<li class="mb-2">
 <a href="https://www.computer.org/csdl/journal/ai/5555/01/10908573/24HXWjMwwww">GeoDCL: Weak Geometrical Distortion based Contrastive Learning for Fine-grained Fashion Image Retrieval</a>, 
  <strong>L. Xiao</strong> and T. Yamasaki,    
 <em><strong>IEEE Transactions on Artificial Intelligence</strong></em>, vol.1, pp. 1-13, 2025.
 </li>
		<li class="mb-2">
 <a href="https://www.sciencedirect.com/science/article/pii/S0950705125000036">Multi-level Knowledge Distillation for Fine-grained Fashion Image Retrieval</a>, 
  <strong>L. Xiao</strong> and T. Yamasaki,    
 <em><strong>Knowledge-Based Systems</strong></em>, vol.310, p.112955, 2025.
 </li>
		
		<li class="mb-2">
 <a href="https://www.sciencedirect.com/science/article/pii/S0950705124011547"> LiFSO-Net: A Lightweight Feature Screening Optimization Network for Complex-scale Flat Metal Defect Detection</a>, 
  Hao Zhong, <strong>L. Xiao</strong>, Haifeng Wang, Xin Zhang, Chenhui Wan, and Bo Wu.   
 <em><strong>Knowledge-Based Systems</strong></em>, vol.304, pp.112520, 2024.
 </li>
<li class="mb-2">
 <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10486897">Attribute-Guided Multi-Level Attention Network for Fine-Grained Fashion Retrieval</a>, 
  <strong>L. Xiao</strong> and T. Yamasaki,   
 <em><strong>IEEE Access</strong></em>, vol.12, pp.48068-48080, 2024 <a href="https://github.com/Dr-LingXiao/AG-MAN"> [Code] </a>
 </li>
	
	<li class="mb-2">
	<a href="https://www.sciencedirect.com/science/article/pii/S1474034624000855"> STFE-Net: A multi-stage approach to enhance statistical texture feature for defect detection on metal surfaces</a>, 
		H. Zhong, D. X. Fu, <strong>L. Xiao</strong>, F. Zhao, J. Liu, B. Wu, and Y. M. Hu,
		<em><strong>Advanced Engineering Informatics</strong></em>, vol.61, pp.102437, 2024 
 </li>
 <li class="mb-2">
 <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200684">Missing small fastener detection using deep learning</a>, 
 <strong>L. Xiao</strong>, B. Wu, and Y. Hu,  
 <em><strong>IEEE Transactions on Instrumentation and Measurement</strong></em>, vol.70, pp.1-9, 2020
 </li>

 <li class="mb-2">
 <a href="https://www.sciencedirect.com/science/article/pii/S104732032030153X">OSED: Object-specific edge detection</a>, 
 <strong>L. Xiao</strong>, B. Wu, and Y. Hu,
 <em><strong>Journal of Visual Communication and Image Representation</strong></em>, vol.72, pp.102918, 2020
 </li>
	
 <li class="mb-2">
 <a href="https://link.springer.com/article/10.1007/s00170-020-05205-0">Detection of powder bed defects in selective laser sintering using convolutional neural network</a>, 
 <strong>L. Xiao</strong>, M. Lu, and H. Huang,
 <em><strong>The International Journal of Advanced Manufacturing Technology</strong></em>, vol.107, pp.2485-2496, 2020
 </li>	

 <li class="mb-2">
 <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8908783">A hierarchical features-based model for freight train defect inspection</a>, 
 <strong>L. Xiao</strong>, B. Wu, Y. Hu, and J. Liu,
 <em><strong>IEEE Sensors Journal</strong></em>, vol.20(5), pp.2671-2678, 2019
 </li>	
	
 <li class="mb-2">
 <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9019620">Surface defect detection using image pyramid</a>, 
 <strong>L. Xiao</strong>, B. Wu, and Y. Hu,
 <em><strong>IEEE Sensors Journal</strong></em>, vol.20(13), pp.7181-7188, 2020
 </li>	
	</ol>
 
</ul>		    
<h4>Arxiv papers</h4>
	<ul>	
		<ol>    
			<li class="mb-2">
 <a href="https://arxiv.org/abs/2503.01236">LLM-Advisor: An LLM Benchmark for Cost-efficient Path Planning across Multiple Terrains</a>, 
 <strong>L. Xiao</strong> and T. Yamasaki,    
 <em><strong>arXiv preprint arXiv:2503.01236.</strong></em>, 2025
 </li>	 
			 <li class="mb-2">
 <a href="https://arxiv.org/abs/2503.01236">LLM-Advisor: An LLM Benchmark for Cost-efficient Path Planning across Multiple Terrains</a>, 
 <strong>L. Xiao</strong> and T. Yamasaki,    
 <em><strong>arXiv:2503.01236</strong></em>, 2025
 </li>	 
			
		 <li class="mb-2">
 <a href="https://arxiv.org/pdf/2405.08890">Language-Guided Self-Supervised Video Summarization Using Text Semantic Matching Considering the Diversity of the Video</a>, 
 T. Sugihara, S. Masuda, <strong>L. Xiao</strong>, and T. Yamasaki,   
 <em><strong>arXiv preprint arXiv:2405.08890</strong></em>, 2024
 </li>	 
		    	<li class="mb-2">
 <a href="https://arxiv.org/pdf/2309.02870">Rethinking Momentum Knowledge Distillation in Online Continual Learning</a>, 
 N. Michel, M. Wang, <strong>L. Xiao</strong>, and T. Yamasaki,   
 <em><strong>arXiv preprint arXiv:2309.02870.</strong></em>, 2023 
 </li>
		    <li class="mb-2">	    
 <a href="https://arxiv.org/pdf/2305.13802">Online Open-set Semi-supervised Object Detection via Semi-supervised Outlier Filtering</a>, 
 Z. Wang, <strong>L. Xiao</strong>, L. Xiang, Z. Weng, and T. Yamasaki,   
 <em><strong>arXiv preprint arXiv:2305.13802.</strong></em>, 2023
 </li>
 <li class="mb-2">
 <a href="https://arxiv.org/pdf/2303.07951.pdf">MetaMixer: A Regularization Strategy for Online Knowledge Distillation</a>, 
 M. Wang, <strong>L. Xiao</strong> and T. Yamasaki,  
 <em><strong>arXiv preprint arXiv:2303.07951.</strong></em>, 2023
 </li>
		
 <li class="mb-2">
 <a href="https://arxiv.org/pdf/2212.14680.pdf">Semi-supervised Fashion Compatibility Prediction by Color Distortion Prediction</a>, 
 <strong>L. Xiao</strong> and T. Yamasaki,  
 <em><strong>arXiv preprint arXiv:2212.14680.</strong></em>, 2022
 </li>
	
 <li class="mb-2">
 <a href="https://arxiv.org/pdf/2301.13014.pdf">Attribute-Guided Multi-Level Attention Network for Fine-Grained Fashion Retrieval</a>, 
 <strong>L. Xiao</strong> and T. Yamasaki,  
 <em><strong>arXiv preprint arXiv:2301.13014.</strong></em>, 2022
 </li>
			</ul>	
		</ol>		
				<h4>Domestic conferences</h4>	
				<ul>	 
					<ol>
							<li class="mb-2">
					 <a href="">ã‚¿ã‚¹ã‚¯é©å¿œçš„æ¤œç´¢æ‹¡å¼µå­¦ç¿’ã«åŸºã¥ãè¦³å…‰ç‰¹åŒ–å¤§è¦æ¨¡ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«</a>, 
					 å±±è¥¿åšé›…, <strong>è‚–ã€€ç²</strong>, å±±å´ä¿Šå½¦
					 <em><strong> ä¿¡å­¦æŠ€å ±, ç”»åƒå·¥å­¦ç ”ç©¶ä¼š (IE)</strong></em>, IE2024-61
					 </li>
						
						<li class="mb-2">
					 <a href="">Explainable Image Aesthetic Assessment Leveraging Vision-Language Models</a>, 
					  S. Viriyavisuthisakul, S.n Yoshida, K. Shiohara, <strong>L. Xiao</strong> and T. Yamasaki,  
					 <em><strong> ä¿¡å­¦æŠ€å ±, ç”»åƒå·¥å­¦ç ”ç©¶ä¼š (IE)</strong></em>, IE2024-66
					 </li>

						
						<li class="mb-2">
					 <a href="">Momentum Knowledge Distillation for Enhanced Online Continual Learning</a>, 
					  N. Michel, M. Wang, <strong>L. Xiao</strong>, and T. Yamasaki,   
					 <em><strong> ä¿¡å­¦æŠ€å ±, ç”»åƒå·¥å­¦ç ”ç©¶ä¼š (IE)</strong></em>, IE2024-57
					 </li>
						
							<li class="mb-2">
					 <a href="">Llava-Planner: Enhancing Spatial Awareness of LLaVA for Cost-Effective Path Planning</a>, 
					  <strong>L. Xiao</strong>, H. Yamanishi, and T. Yamasaki,
					 <em><strong> ä¿¡å­¦æŠ€å ±, ç”»åƒå·¥å­¦ç ”ç©¶ä¼š (IE)</strong></em>, IE2024-44
					 </li>
						
                                                <li class="mb-2">
						 <a href="">LLM-Advisor: A LLM Benchmark for Cost-effective Path Planning</a>, 
						  <strong>L. Xiao</strong> and T. Yamasaki,   
						 <em><strong> PCSJ/IMPS2024 </strong></em>, P-2-05, 2024 </a>
						 </li>
						      <li class="mb-2">
						 <a href="">ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«è¦³å…‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨å¤§è¦æ¨¡ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ</a>, 
						  H. Yamanishi, <strong>L. Xiao</strong>, and T. Yamasaki,     
						 <em><strong> PCSJ/IMPS2024 </strong></em>, P-4-18, 2024 </a>
						 </li>
						
						<li class="mb-2">
						 <a href="">Boosting Fine-grained Fashion Retrieval with Relational Knowledge Distillation</a>, 
						  <strong>L. Xiao</strong> and T. Yamasaki,   
						 <em><strong> ä¿¡å­¦æŠ€å ±, ç”»åƒå·¥å­¦ç ”ç©¶ä¼š (IE) </strong></em>, vol. 124, no. 60, IE2024-17, pp. 90-94, 2024 <a href="https://github.com/Dr-LingXiao/RKD"> [Code] </a>
						 </li>
						<li class="mb-2">
					 <a href="">Language-Guided Self-Supervised Video Summarization Using Text Semantic Matching</a>, 
					 T. Sugihara, S. Masuda, <strong>L. Xiao</strong>, and T. Yamasaki,  
					 <em><strong> The 27th Meeting on Image Recognition and Understanding (MIRU)</strong></em>, 2024. [Oral]
					 </li>

							<li class="mb-2">
					 <a href="">å¤§è¦æ¨¡ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸåºƒå‘Šç”»åƒã®è©•ä¾¡ãƒ»æ”¹å–„</a>, 
					 ç ‚ç”° é”å·³, å¡©åŸ æ¥“, åŠ‰ å²³æ¾, ä¸¹æ²» ç›´äºº, å‹¢ã€† å¼˜å¹¸, <strong>è‚–ç²</strong>,å±±å´ ä¿Šå½¦,
					 <em><strong> The 27th Meeting on Image Recognition and Understanding (MIRU)</strong></em>, 2024. [Oral]
					 </li>

						<li class="mb-2">
					 <a href="">Multi-hop Question Answering over Incomplete Knowledge Graphs by Edge and Meaning Extensions</a>, 
					 X.T. Ye, <strong>L. Xiao</strong>, C. Zhang, and T. Yamasaki,   
					 <em><strong> The 27th Meeting on Image Recognition and Understanding (MIRU)</strong></em>, 2024. 
					 </li>

						<li class="mb-2">
					 <a href="">Constrianed advertisement layout generation based on Graph Neural Networks</a>, 
					 C. Fu, Y. Liu, N. Tanji, H. Seshime, S. Yi, <strong>L. Xiao</strong>, and T. Yamasaki,  
					 <em><strong> The 27th Meeting on Image Recognition and Understanding (MIRU)</strong></em>, 2024. 
					 </li>
						
					<li class="mb-2">
					 <a href="">Improving Adversarial Robustness in Continual Learning</a>, 
					 K. Mukai, S. Kumano, N. Michel, <strong>L. Xiao</strong>, and T. Yamasaki,   
					 <em><strong> ä¿¡å­¦æŠ€å ±, ç”»åƒå·¥å­¦ç ”ç©¶ä¼š (IE)</strong></em>, vol.123, no.381, IE2023-37, pp.13-18, 2024. [IEè³]
					 </li>
					<li class="mb-2">
					 <a href="">å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ãŸè‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ã«ã‚ˆã‚‹ãƒ“ãƒ‡ã‚ªè¦ç´„</a>, 
					 æ‰åŸæœ‹å¼¥, å¢—ç”°ä¿Šå¤ªéƒ, <strong>è‚–ç²</strong>, å±±å´ä¿Šå½¦,   
					 <em><strong> IPSJ</strong></em>, 7T-06, pp.2-653-2-654, 2024. 
					 </li>
						<li class="mb-2">
					 <a href="">Advertisement Layout Generation based on Graph Neural Network</a>, 
					 C. Fu, Y. Liu, N. Tanji, H. Seshime, <strong>L. Xiao</strong>, and T. Yamasaki,   
					 <em><strong> ä¿¡å­¦æŠ€å ±, ç”»åƒå·¥å­¦ç ”ç©¶ä¼š (IE)</strong></em>, vol. 123, no. 381, IE2023-51, pp. 88-89, 2024.
					 </li>
						
					<li class="mb-2">
					 <a href="">Improved Fine-grained Fashion Retrieval with Contrastive Learning</a>, 
					 <strong>L. Xiao</strong>, X. F. Zhang, and T. Yamasaki,   
					 <em><strong> The 26th Meeting on Image Recognition and Understanding (MIRU)</strong></em>, IS3-55, 2023.
					 </li>
					
					<li class="mb-2">
					 <a href="">Video Summarization Based on Masked Autoencoder</a>, 
					 M. L. A. FOK, <strong>L. Xiao</strong>, and T. Yamasaki,   
					 <em><strong> The 26th Meeting on Image Recognition and Understanding (MIRU)</strong></em>, IS1-84, 2023.
					 </li>
					
					<li class="mb-2">
					 <a href="">Improving Fashion Compatibility Prediction with Color Distortion Prediction</a>, 
					 <strong>L. Xiao</strong>, and T. Yamasaki,   
					 <em><strong> ä¿¡å­¦æŠ€å ±, ç”»åƒå·¥å­¦ç ”ç©¶ä¼š (IE)</strong></em>, vol. 122, no. 385, IE2022-61, pp. 17-18, 2023.
					 </li>
					 <li class="mb-2">
					 <a href="">Multi-Level Attention Network for Fine-Grained Fashion Retrieval</a>, 
					 <strong>L. Xiao</strong> and T. Yamasaki,  
					 <em><strong> ä¿¡å­¦æŠ€å ±, MVE</strong></em>, vol. 122, no. 440, MVE2022-90, pp. 198-199, 2023
					 </li>
					 <li class="mb-2">
					 <a href="">SAT: Self-adaptive training for fashion compatibility prediction</a>, 
					 <strong>L. Xiao</strong> and T. Yamasaki,  
					 <em><strong> The 25th Meeting on Image Recognition and Understanding (MIRU)</strong></em>, 2022
					 </li>
				        <li class="mb-2">
						<a href="">Spatial Attention Based Fashion Compatibility Prediction</a>, 
						<strong>L. Xiao</strong> and T. Yamasaki,  
						<em><strong>PCSJ/IMPS2021</strong></em>, P-3-17, pp. 135-136, 2021

					</li>

                    			<li class="mb-2">
						<a href="">Design and Analysis of Attention-Aware Embedding Network for Fashion Compatibility Prediction</a>, 
						<strong>L. Xiao</strong> and T. Yamasaki,  
						<em><strong>ITE å†¬å­£å¤§ä¼š</strong></em>, 21A-6, 2021
					</li>
					</ol>
                    			
			</ul>
				<h4>Patents (China)</h4>
						<ul>
							<ol>
													 <li class="mb-2">
						 <a href="">ä¸€ç§ç”¨äºé™è„‰ç©¿åˆºçš„ç©¿åˆºé¶ç‚¹è¯†åˆ«ä¸å®šä½æ–¹æ³•</a>, 
						 <strong>è‚–ç²</strong>ã€æ¬§é˜³æµ©ã€å¶éœ–ã€éŸ©æ–Œã€é™ˆå­¦ä¸œã€æ¨æ–°
						 <em><strong>å‘æ˜ä¸“åˆ© ä¸“åˆ©å·: 202210202422 .1 ç”³è¯·ä¸­</strong></em>
						 </li>
											 <li class="mb-2">
						 <a href="">ä¸€ç§é’¢å·åŒç›®è§†è§‰å®šä½æ–¹æ³•åŠè®¾å¤‡</a>, 
						 èƒ¡å‹æ°‘ã€<strong>è‚–ç²</strong>ã€å´æ³¢
						 <em><strong>å‘æ˜ä¸“åˆ© ä¸“åˆ©å·: 201810094718 .X, æˆæƒæ—¥ï¼š2020.09.18</strong></em>
						 </li>
								 <li class="mb-2">
						 <a href="">ä¸€ç§åŸºäºè§†è§‰çš„é’¢å·å®šä½æ–¹æ³•åŠè®¾å¤‡</a>, 
						 èƒ¡å‹æ°‘ã€<strong>è‚–ç²</strong>ã€å´æ³¢
						 <em><strong>å‘æ˜ä¸“åˆ© ä¸“åˆ©å·: 201811059328 .5, æˆæƒæ—¥ï¼š2020.07.10</strong></em>
						 </li>
								
						 <li class="mb-2">
						 <a href="">ä¸€ç§å¯è§†åŒ–çš„èµ·é‡æœºåŠå–å®šä½ç³»ç»Ÿ</a>, 
						 èƒ¡å‹æ°‘ã€<strong>è‚–ç²</strong>ã€å´æ³¢ã€åˆ˜é¢‰ 
						 <em><strong>å‘æ˜ä¸“åˆ© ä¸“åˆ©å·: 201611246219.5, æˆæƒæ—¥ï¼š2018.01.02</strong></em>
						 </li>

						 <li class="mb-2">
						 <a href="">ä¸€ç§ç„Šæ¥ç†”æ± åŠ¨æ€è¿‡ç¨‹çš„åœ¨çº¿ç›‘æµ‹ç³»ç»ŸåŠæ–¹æ³•</a>, 
						 èƒ¡å‹æ°‘ã€åˆ˜é¢‰ã€<strong>è‚–ç²</strong>ã€å”æ¾ã€è°·å‹‡
						 <em><strong>å‘æ˜ä¸“åˆ©ï¼Œä¸“åˆ©å·: 201610288460.8, æˆæƒæ—¥ï¼š2018.06.12</strong></em>
						 </li>

						 <li class="mb-2">
						 <a href="">ä¸€ç§ç”¨äºç„Šæ¥ç†”æ± åœ¨çº¿ç›‘æµ‹å¹³å°çš„å¤šåŠŸèƒ½å¤¹å…·</a>, 
						 èƒ¡å‹æ°‘ã€å”æ¾ã€<strong>è‚–ç²</strong>ã€è°·å‹‡ã€åˆ˜é¢‰
						 <em><strong>å®ç”¨æ–°å‹ä¸“åˆ©ï¼Œä¸“åˆ©å·: 201620434683.6, æˆæƒæ—¥ï¼š2016.10.05</strong></em>
						 </li>	
								 <li class="mb-2">
						 <a href="">ä¸€ç§é’ˆå¯¹å…‰æµå›¾çš„å¿«é€Ÿçš„FCMå›¾åƒåˆ†å‰²æ–¹æ³•</a>, 
						 èƒ¡å‹æ°‘ã€èƒ¡ä¸­æ—­ã€å´æ³¢ã€æ­¦æ•å¥ã€åˆ˜é¢‰ã€<strong>è‚–ç²</strong>ã€ç‹è¯—æ°ã€æé›ªè²
						 <em><strong>å‘æ˜ä¸“åˆ©ï¼Œä¸“åˆ©å·: 201710530461.3, æˆæƒæ—¥ï¼š2019.11.12</strong></em> 
									 <strong>(2023å¹´åº¦æ¹–åŒ—çœç§‘å­¦æŠ€æœ¯å¥–æå)</strong>
						 </li>	
								
							</ol>
						</ul>	
		    


            </div>
        </div>



    </div>


    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
  </body>
</html>
